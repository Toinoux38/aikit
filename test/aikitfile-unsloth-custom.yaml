#syntax=aikit:test
apiVersion: v1alpha1
debug: true
runtime: cuda
models:
  - name: custom
    source: model-q4_k_m.gguf
config: |
  - name: custom
    backend: llama
    parameters:
      top_k: 80
      temperature: 0.2
      top_p: 0.7
      model: model-q4_k_m.gguf
    context_size: 4096
    gpu_layers: 35
    f16: true
    batch: 512
    mmap: true
