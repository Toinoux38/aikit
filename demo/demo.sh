#!/bin/bash

. third_party/demo-magic/demo-magic.sh

clear
DEMO_PROMPT="${GREEN}âœ  ${COLOR_RESET}"

# echo "âœ¨ In this demo, we are going to start by fine tuning a model and then deploy the model as a minimal container!"

# echo ""

# echo "ğŸ‘·â€â™€ï¸ First, we are going to create a new builder"

# echo ""

# pei "docker buildx create --name aikit-builder --use --buildkitd-flags '--allow-insecure-entitlement security.insecure'"

# echo ""

# echo "ğŸ—ƒï¸ Create a configuration for the fine tuning. We are going to be using a LLama 2 model and fine tune using yahma/alpaca-cleaned dataset."

# cat > aikit-finetune.yaml << EOF
# #syntax=sozercan/aikit:latest
# apiVersion: v1alpha1
# baseModel: unsloth/llama-2-7b-bnb-4bit
# datasets:
#   - source: "yahma/alpaca-cleaned"
#     type: alpaca
# config:
#   unsloth:
# EOF

# echo ""

# pei "bat aikit-finetune.yaml"

# echo ""

# echo "ğŸµ Starting the fine tuning process using the above configuration file, and output fine tuned model will be saved in _output folder."

# echo ""

# pei "docker buildx build --builder aikit-builder --allow security.insecure --file 'aikit-finetune.yaml' --output '_output' --target unsloth --progress plain ."

# echo ""

# echo "âœ… We have finished fine tuning the model. Let's look at the output..."

# echo ""

# pei "ls -al _output"

# echo ""

# echo "ğŸ“¦ Now that we have a fine tuned model. We can deploy it as a minimal container."

# echo ""

# echo "ğŸ“ƒ We'll start by creating a basic inference configuration file for the deployment."

# cat <<EOF >> aikit-inference.yaml
# #syntax=sozercan/aikit:latest
# apiVersion: v1alpha1
# models:
#   - name: llama-2-finetuned
#     source: aikit-model-q4_k_m.gguf
# EOF

# pei "bat aikit-inference.yaml"

# echo ""

# echo "ğŸ—ï¸ We can now build a minimal container for the model using the configuration file."

# echo ""

# pei "docker buildx build -t llama-finetuned -f aikit-inference.yaml --load _output"

# echo ""

echo "ğŸƒ We have finished building the minimal container. Let's start the container and test it."

pei "docker run --name llama-2-finetuned -d --rm -p 8080:8080 llama-finetuned"

echo ""

echo "ğŸ§ª We can now test the container using a sample query. Since this is OpenAI API compatible, you can use it as a drop-in replacement for any application that uses OpenAI API."

echo ""

pei "curl http://localhost:8080/v1/chat/completions -H 'Content-Type: application/json' -d \"{'model': 'llama-2-finetuned', 'messages': [{'role': 'user', 'content': 'Compose a haiku about cats'}]}\""

# echo ""

# echo "ğŸ™Œ We have successfully deployed the fine tuned model as a minimal container and successfully verified it! We can now stop the container if we wish."

# echo ""

# pei "docker stop llama-2-finetuned"

# echo ""

# echo "â¤ï¸ In this demo, we have shown how to fine tune a model and deploy it as a minimal container using AIKit. Thank you for watching!"

# pei "docker buildx rm aikit-builder"
